{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee1Zu51qfIqI",
        "outputId": "516c1c5e-e30c-4272-830f-0a3c8ebfe091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmcRMWdj5D4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa31f698-6af3-4e48-9832-8415d26fc0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 29 07:14:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZdM_tYDpiA4"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.19.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PAJNhrvSvgFS",
        "outputId": "4fdfcf61-fc12-4de2-cce6-2970c6004b87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.19.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy\n",
        "numpy.version.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMTpRw54xJ-j"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "import cv2\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, img_res=(256, 256)):        \n",
        "        self.img_res = img_res\n",
        "        \n",
        "    def load_batch(self, batch_size=1, is_testing=False):        \n",
        "        path = glob('/content/drive/MyDrive/Model/training_png/*')\n",
        "       \n",
        "        self.n_batches = int(len(path) / batch_size)\n",
        "\n",
        "        for i in range(self.n_batches - 1):\n",
        "            batch = path[i * batch_size:(i + 1) * batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img in batch:\n",
        "              \n",
        "                img_r = cv2.imread(img)\n",
        "                # Transform from bgr to rgb\n",
        "                img_r = cv2.cvtColor(img_r, cv2.COLOR_BGR2RGB)\n",
        "                h, w, _ = img_r.shape\n",
        "                half_w = int(w / 2)\n",
        "                img_A = img_r[:, :half_w, :]\n",
        "                img_B = img_r[:, half_w:, :]\n",
        "                # Resize to 256,256\n",
        "                img_A = resize(img_A, self.img_res)\n",
        "                img_B = resize(img_B, self.img_res)\n",
        "                # Append to list             \n",
        "                imgs_A.append(img_A/255.)\n",
        "                imgs_B.append(img_B/255.)\n",
        "\n",
        "            imgs_A = np.array(imgs_A)\n",
        "            imgs_B = np.array(imgs_B)\n",
        "\n",
        "            yield imgs_A, imgs_B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E82XdkWlxLMn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import ZeroPadding2D, Concatenate\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "from tensorflow.keras.layers import PReLU\n",
        "class Discriminator():\n",
        "    def __init__(self,img_shape=(256,256,3)):\n",
        "        self.img_shape = img_shape\n",
        "    \n",
        "    # Pyramidal Pooling\n",
        "    def Pyramid_Pool(self, layer_input):\n",
        "        x_list = [layer_input]\n",
        "\n",
        "        def Pool():\n",
        "            x = MaxPooling2D(pool_size=(2, 2))(layer_input)           \n",
        "            x = self.Deconv2d_block(x, 2)\n",
        "            return x\n",
        "\n",
        "        # First level\n",
        "        x_list.append(Pool())\n",
        "        # Second level\n",
        "        x2 = MaxPooling2D(pool_size=(4, 4))(layer_input)\n",
        "        x2 = self.Deconv2d_block(x2, 2)\n",
        "        x2 = self.Deconv2d_block(x2, 2)\n",
        "        x2 = ZeroPadding2D(padding=(1, 1))(x2)\n",
        "        x_list.append(x2)\n",
        "        # Third level\n",
        "        x3 = MaxPooling2D(pool_size=(8, 8))(layer_input)\n",
        "        x3 = self.Deconv2d_block(x3, 4)\n",
        "        x3 = self.Deconv2d_block(x3, 4)\n",
        "        x3 = self.Deconv2d_block(x3, 4)\n",
        "        x3 = ZeroPadding2D(padding=(3, 3))(x3)\n",
        "        x_list.append(x3)\n",
        "\n",
        "        x = Concatenate(axis=-1)(x_list)\n",
        "        return x\n",
        "\n",
        "    # Discriminator Layer\n",
        "    def d_layer(self, layer_input, filters, kernel=4, bn=True):\n",
        "\n",
        "        x = Conv2D(filters, kernel_size=kernel, strides=1)(layer_input)\n",
        "        x = PReLU()(x)\n",
        "        if bn:\n",
        "            x = BatchNormalization(momentum=0.8)(x)\n",
        "        x = MaxPooling2D()(x)\n",
        "        return x\n",
        "\n",
        "    # Deconv Layer\n",
        "    def Deconv2d_block(self, layer_input, filters, kernel=4):\n",
        "        x = UpSampling2D(size=2)(layer_input)\n",
        "        x = Conv2D(filters, kernel_size=kernel, strides=1, padding='same', activation='relu')(x)        \n",
        "        x = BatchNormalization(momentum=0.8)(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "\n",
        "    def create_discriminator(self):\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
        "\n",
        "        x0 = self.d_layer(combined_imgs, 64, 3)\n",
        "        x1 = self.d_layer(x0, 256, 3)\n",
        "        x2 = self.d_layer(x1, 512, 3)\n",
        "        x3 = self.d_layer(x2, 64, 3)\n",
        "        x4 = self.Pyramid_Pool(x3)\n",
        "        # Output c is 72\n",
        "        out = Activation('sigmoid')(x4)\n",
        "\n",
        "        return Model([img_A, img_B], out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlFZIQOQxMi2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import ZeroPadding2D, Concatenate\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, img_shape = (256,256,3)):\n",
        "        self.img_shape = img_shape\n",
        "    \n",
        "    # Dense Block with skip connection\n",
        "    def dense_block(self, layer_input, num_layers):  \n",
        "        x_list = [layer_input]\n",
        "        for i in range(num_layers):\n",
        "            x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(layer_input)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            \n",
        "            x_list.append(x)\n",
        "            x = Concatenate(axis=-1)(x_list) \n",
        "        return x    \n",
        "    \n",
        "    def Conv2d_block(self, layer_input, n_filters, kernel, stride, padding='valid'):  # Generator Convolution Layer  \n",
        "        x = Conv2D(filters=n_filters, kernel_size=kernel, strides=stride, padding=padding)(layer_input)\n",
        "        x = BatchNormalization(momentum=0.8)(x)\n",
        "        x = Activation('relu')(x)\n",
        "        \n",
        "        return x    \n",
        "\n",
        "    def Deconv2d_block(self, layer_input, n_filters):\n",
        "        x = UpSampling2D(size=2)(layer_input)\n",
        "        x = Conv2D(n_filters, kernel_size=4, strides=1, padding='same', activation='relu')(x)\n",
        "        x = BatchNormalization(momentum=0.8)(x)\n",
        "        return x\n",
        "\n",
        "    def create_generator(self):\n",
        "        inp = Input(shape=self.img_shape)\n",
        "        \n",
        "        # DownSampling \n",
        "        x0 = self.Conv2d_block(inp, 64, (3, 3), (1, 1))\n",
        "        x0 = MaxPooling2D()(x0)\n",
        "\n",
        "        x1 = self.dense_block(x0, 4)\n",
        "        x1 = self.Conv2d_block(x1, 128, (3, 3), (2, 2))\n",
        "\n",
        "        x2 = self.dense_block(x1, 6)\n",
        "        x2 = self.Conv2d_block(x2, 256, (3, 3), (2, 2))\n",
        "\n",
        "        # No sampling\n",
        "        x3 = self.dense_block(x2, 8)\n",
        "        x3 = self.Conv2d_block(x3, 512, (3, 3), (1, 1), padding='same')\n",
        "\n",
        "        x4 = self.dense_block(x3, 8)\n",
        "        x4 = self.Conv2d_block(x4, 512, (3, 3), (1, 1), padding='same')\n",
        "\n",
        "        x5 = self.dense_block(x4, 8)\n",
        "        x5 = self.Conv2d_block(x5, 128, (3, 3), (1, 1), padding='same')\n",
        "\n",
        "      # UpSampling \n",
        "        x6 = self.dense_block(x5, 6)\n",
        "        x6 = self.Deconv2d_block(x6, 120)\n",
        "\n",
        "        x7 = self.dense_block(x6, 4)\n",
        "        x7 = self.Deconv2d_block(x7, 64)\n",
        "\n",
        "        x8 = self.dense_block(x7, 4)\n",
        "        x8 = self.Deconv2d_block(x8, 64)\n",
        "\n",
        "        x9 = self.dense_block(x8, 4)\n",
        "        x9 = self.Conv2d_block(x9, 16, (3, 3), (1, 1), padding='same')\n",
        "\n",
        "        x10 = ZeroPadding2D(padding=(5, 5))(x9)\n",
        "\n",
        "        x11 = Conv2D(filters=3, kernel_size=(3, 3))(x10)\n",
        "        out = Activation('tanh')(x11)\n",
        "\n",
        "        return Model(inp, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIQFhvfjxN7x"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "import cv2\n",
        "import pandas\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import ZeroPadding2D, Concatenate\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Conv2DTranspose\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from tensorflow.keras.layers import PReLU\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.misc\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_shape = (256,256,3)\n",
        "        \n",
        "        \n",
        "        # Instance of data loader\n",
        "        self.data_loader = DataLoader(img_res=(256,256))\n",
        "        \n",
        "        self.disc_out = (14, 14, 72)  \n",
        "\n",
        "        self.generator = Generator().create_generator()\n",
        "        self.discriminator = Discriminator().create_discriminator()\n",
        "        self.GAN_model = self.build_GAN() \n",
        "         \n",
        "         \n",
        "        self.gan_optimizer = Adam(learning_rate=1E-3, beta_1=0.3, beta_2=0.99, epsilon=1e-08,amsgrad=True)         \n",
        "        self.discriminator_optimizer = Adam(learning_rate=1E-3, beta_1=0.5, beta_2=0.99, epsilon=1e-08,amsgrad=True)\n",
        "\n",
        "    def build_GAN(self):\n",
        "        self.discriminator.trainable = False\n",
        "        \n",
        "        img_B = Input(shape=self.img_shape)       \n",
        "        fake_A = self.generator(img_B)\n",
        "        \n",
        "        discriminator_output = self.discriminator([fake_A, img_B])\n",
        "        \n",
        "        GAN_model = Model(inputs=[img_B],\n",
        "                           outputs=[fake_A, fake_A, discriminator_output],\n",
        "                           name='CGAN')  \n",
        "        return GAN_model\n",
        "\n",
        "    def train(self, epochs, batch_size=10, sample_interval=80):              \n",
        "        def perceptual_loss(img_true, img_generated):  \n",
        "            image_shape = self.img_shape\n",
        "            vgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n",
        "            \n",
        "            loss_block3 = Model(vgg.input, vgg.get_layer('block3_conv3').output)\n",
        "            loss_block3.trainable = False\n",
        "            loss_block2 = Model(vgg.input, vgg.get_layer('block2_conv2').output)\n",
        "            loss_block2.trainable = False\n",
        "            loss_block1 = Model(vgg.input, vgg.get_layer('block1_conv2').output)\n",
        "            loss_block1.trainable = False\n",
        "            return np.mean(K.square(loss_block1(img_true) - loss_block1(img_generated))) + K.mean(\n",
        "                K.square(loss_block2(img_true) - loss_block2(img_generated))) + K.mean(\n",
        "                K.square(loss_block3(img_true) - loss_block3(img_generated)))\n",
        "        \n",
        "        # Set the Discriminator to false for training Generator  \n",
        "        self.discriminator.trainable = False  \n",
        "        # Compile the Generator\n",
        "        self.generator.compile(loss=perceptual_loss, optimizer=self.gan_optimizer,run_eagerly=True)  \n",
        "        # Three Loses\n",
        "        CGAN_loss = ['mae', perceptual_loss, 'mse']  \n",
        "        CGAN_loss_weights = [6.6e-3, 1, 6.6e-3]\n",
        "        self.GAN_model.compile(loss=CGAN_loss, loss_weights=CGAN_loss_weights,\n",
        "                                optimizer=self.gan_optimizer,run_eagerly=True)\n",
        "        \n",
        "        # To train the Discriminator set trainable to true\n",
        "        \n",
        "        self.discriminator.trainable = True\n",
        "        self.discriminator.compile(loss=\"mse\",\n",
        "                                   optimizer=self.discriminator_optimizer)\n",
        "        start_time = datetime.datetime.now()\n",
        "        # Real world Images\n",
        "        \n",
        "        valid = np.ones((batch_size,) + self.disc_out) \n",
        "        # Generated Images\n",
        "        fake = np.zeros((batch_size,) + self.disc_out)  \n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            if epoch == 0:\n",
        "              try: \n",
        "                self.generator.load_weights(\"/content/drive/MyDrive/Model/gen_model_v2.h5\")\n",
        "                self.discriminator.load_weights('/content/drive/MyDrive/Model/dis_model_v2.h5')\n",
        "                self.GAN_model.load_weights('/content/drive/MyDrive/Model/com_model_v2.h5')\n",
        "              except:\n",
        "                pass\n",
        "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "                # Generated Image\n",
        "                fake_A = self.generator.predict(imgs_B)  \n",
        "                \n",
        "                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
        "                d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
        "                # Discriminator Loss\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)  \n",
        "                # Train the Combined model\n",
        "                self.GAN_model.trainable = True  \n",
        "                self.discriminator.trainable = False\n",
        "                g_loss = self.GAN_model.train_on_batch(imgs_B, [imgs_A, imgs_A, valid])\n",
        "\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "                if batch_i % sample_interval == 0:\n",
        "                  print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\" % (epoch, epochs,\n",
        "                                                                                          batch_i,\n",
        "                                                                                          self.data_loader.n_batches,\n",
        "                                                                                          d_loss,\n",
        "                                                                                          g_loss[0],\n",
        "                                                                                          elapsed_time))\n",
        "\n",
        "                  \n",
        "                  imgshow = np.zeros((256,512,3))\n",
        "                  imgshow[:,:256,:] = (imgs_A[0,:,:,:]  * 255.)\n",
        "                  imgshow[:,256:,:] = (fake_A[0,:,:,:]  * 255.)\n",
        "                  plt.imshow(imgshow)\n",
        "                  plt.show()\n",
        "\n",
        "            # Save weights after each epoch\n",
        "            self.GAN_model.save_weights(\"/content/drive/MyDrive/Model/com_model_v2.h5\")\n",
        "            self.generator.save_weights(\"/content/drive/MyDrive/Model/gen_model_v2.h5\")\n",
        "            self.discriminator.save_weights(\"/content/drive/MyDrive/Model/dis_model_v2.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzIwZYqJzNMi"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  gan = GAN()\n",
        "  gan.train(epochs=500, batch_size=10, sample_interval=80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gan = GAN()\n",
        "gan.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "B8kz6_j2UHg6",
        "outputId": "c250e58e-d432-49b5-d9e8-c9e2b4ed8edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2d115f90c9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'GAN' object has no attribute 'get_weights'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZtVabzXECOg"
      },
      "outputs": [],
      "source": [
        "from math import log10, sqrt\n",
        "def PSNR(original, compressed):\n",
        "    mse = np.mean((original - compressed) ** 2)\n",
        "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
        "                  # Therefore PSNR have no importance.\n",
        "        return 100\n",
        "    max_pixel = 255.0\n",
        "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
        "    return psnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBJpSgduKok5"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import mean_squared_error as mse\n",
        "from google.colab.patches import cv2_imshow\n",
        "psnrs = []\n",
        "ssims = []\n",
        "path = glob(\"/content/drive/MyDrive/Model/testing/*\")\n",
        "gan=GAN()\n",
        "test_model = gan.generator\n",
        "test_model.load_weights(\"/content/drive/MyDrive/Model/gen_model_v2.h5\")\n",
        "for i in range(len(path)):\n",
        "  img_B = plt.imread(path[i])  \n",
        "  try:\n",
        "    h, w, d = img_B.shape\n",
        "    half_w = int(w/2)\n",
        "    img_show = np.zeros((h,2*w, d))\n",
        "  except:\n",
        "    continue\n",
        "  img_b = img_B[:, half_w:, :] \n",
        "  ground_t = img_B[:, :half_w, :] \n",
        "\n",
        "  ground_t = cv2.resize(ground_t, (256,256)) \n",
        "  img_b = cv2.resize(img_b, (256,256)) \n",
        "\n",
        "  # plt.imshow(ground_t)\n",
        "  # plt.show()\n",
        "  img_b = np.expand_dims(img_b, axis=0)\n",
        "  img_b_norm = np.array(img_b) / 255.\n",
        "\n",
        "  fake = test_model.predict(img_b_norm)* 255.\n",
        "\n",
        "  score = ssim(fake[0,:,:,:], ground_t, multichannel=True)\n",
        "  # ssims.append(score)\n",
        "  # psnrs.append(PSNR(fake,ground_t))\n",
        "  img_show = np.zeros((256,3*256,3))\n",
        "  img_show[:,:256,:] = img_b[0,:,:,:]\n",
        "  img_show[:,256:512,:] = fake[0,:,:,:]\n",
        "  img_show[:,512:,:] = ground_t\n",
        "\n",
        "  # plt.imshow((fake[0,:,:,:]))\n",
        "  cv2.imwrite(f'/content/drive/MyDrive/Model/results/{i}.png',255*img_show)\n",
        "  print(img_show.shape)\n",
        "  plt.imshow(img_show)\n",
        "  plt.show()\n",
        "\n",
        "  print(\"SSIM - \",score)\n",
        "  print(\"PSNR - \",PSNR(fake,ground_t))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  print(\"Mean SSIM - \", np.mean(np.array(ssims)))\n",
        "  print(\"Mean PSNR - \", np.mean(np.array(psnrs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubym_TrumEza",
        "outputId": "4dbe4318-c6d7-40b1-e7c4-de087b15a3f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean SSIM -  0.6064853547339741\n",
            "Mean PSNR -  61.5259045045339\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}